import os
from openai import OpenAI
from pathlib import Path
import tiktoken
import json
import time
from typing import List, Dict, Optional

# naver_mail_parser import ì¶”ê°€
from lib.naver_mail_parser import NaverMailParser

class TextSummarizer:
    def __init__(self, api_key: str = None, model: str = "gpt-3.5-turbo"):
        """
        í…ìŠ¤íŠ¸ ìš”ì•½ê¸° ì´ˆê¸°í™”

        Args:
            api_key: OpenAI API í‚¤ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYì—ì„œ ìë™ ë¡œë“œ)
            model: ì‚¬ìš©í•  ëª¨ë¸ (gpt-3.5-turbo, gpt-4, etc.)
        """
        self.api_key = api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê³µí•˜ì„¸ìš”.")

        self.client = OpenAI(api_key=self.api_key)
        self.model = model
        self.encoding = tiktoken.encoding_for_model(model)
        
        # ëª¨ë¸ë³„ í† í° ì œí•œ
        self.token_limits = {
            "gpt-3.5-turbo": 4096,
            "gpt-3.5-turbo-16k": 16384,
            "gpt-4": 8192,
            "gpt-4-32k": 32768,
            "gpt-4-turbo": 128000,
            "gpt-4o": 128000
        }
        
        self.max_tokens = self.token_limits.get(model, 4096)
    
    def count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.encoding.encode(text))
    
    def read_text_file(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›)"""
        encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as file:
                    return file.read()
            except UnicodeDecodeError:
                continue
        
        raise ValueError(f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    
    def split_text_by_tokens(self, text: str, max_chunk_tokens: int = 3000) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸°ì¤€ìœ¼ë¡œ ë¶„í• """
        sentences = text.split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            test_chunk = current_chunk + sentence + ". "
            if self.count_tokens(test_chunk) > max_chunk_tokens:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
            else:
                current_chunk = test_chunk
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def summarize_chunk(self, text: str, summary_type: str = "detailed") -> str:
        """í…ìŠ¤íŠ¸ ì²­í¬ ìš”ì•½"""
        prompts = {
            "brief": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:",
            "detailed": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìì„¸íˆ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ê³¼ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì•½í•˜ì„¸ìš”:",
            "bullet": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶ˆë › í¬ì¸íŠ¸ í˜•íƒœë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:",
            "korean": "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ì„ ë†“ì¹˜ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•˜ì„¸ìš”:"
        }

        prompt = prompts.get(summary_type, prompts["detailed"])

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°„ê²°í•œ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": f"{prompt}\n\n{text}"}
                ],
                max_tokens=1000,
                temperature=0.3
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            return f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
    
    def summarize_file(self, file_path: str, summary_type: str = "detailed", 
                      output_file: str = None) -> Dict:
        """íŒŒì¼ ìš”ì•½"""
        try:
            # íŒŒì¼ ì½ê¸°
            text = self.read_text_file(file_path)
            file_size = len(text)
            token_count = self.count_tokens(text)
            
            print(f"íŒŒì¼ í¬ê¸°: {file_size:,} ë¬¸ì")
            print(f"í† í° ìˆ˜: {token_count:,}")
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í• 
            if token_count > self.max_tokens - 1000:  # ì‘ë‹µìš© í† í° ì—¬ìœ ë¶„
                print("í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
                chunks = self.split_text_by_tokens(text, max_chunk_tokens=3000)
                
                summaries = []
                for i, chunk in enumerate(chunks, 1):
                    print(f"ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ì¤‘...")
                    summary = self.summarize_chunk(chunk, summary_type)
                    summaries.append(summary)
                    time.sleep(1)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                
                # ì „ì²´ ìš”ì•½ ìƒì„±
                combined_summary = "\n\n".join(summaries)
                if self.count_tokens(combined_summary) > 3000:
                    final_summary = self.summarize_chunk(combined_summary, summary_type)
                else:
                    final_summary = combined_summary
                
            else:
                print("ë‹¨ì¼ ìš”ì•½ ìƒì„± ì¤‘...")
                final_summary = self.summarize_chunk(text, summary_type)
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                "file_path": file_path,
                "original_size": file_size,
                "original_tokens": token_count,
                "summary": final_summary,
                "summary_tokens": self.count_tokens(final_summary),
                "compression_ratio": round(self.count_tokens(final_summary) / token_count * 100, 2)
            }
            
            # íŒŒì¼ë¡œ ì €ì¥
            if output_file:
                self.save_summary(result, output_file)
            
            return result
            
        except Exception as e:
            return {"error": f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"}
    
    def save_summary(self, result: Dict, output_file: str):
        """ìš”ì•½ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        output_path = Path(output_file)

        if output_path.suffix.lower() == '.json':
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        else:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(f"íŒŒì¼: {result['file_path']}\n")
                f.write(f"ì›ë³¸ í¬ê¸°: {result['original_size']:,} ë¬¸ì\n")
                f.write(f"ì›ë³¸ í† í°: {result['original_tokens']:,}\n")
                f.write(f"ìš”ì•½ í† í°: {result['summary_tokens']:,}\n")
                f.write(f"ì••ì¶•ë¥ : {result['compression_ratio']}%\n")
                f.write("=" * 50 + "\n\n")
                f.write(result['summary'])

    def summarize_email(self, email_data: Dict, summary_type: str = "detailed",
                       include_attachments: bool = True, only_attachments: bool = False) -> Dict:
        """
        íŒŒì‹±ëœ ë©”ì¼ ë°ì´í„°ë¥¼ ì§ì ‘ ìš”ì•½

        Args:
            email_data: NaverMailParserì—ì„œ íŒŒì‹±ëœ ì´ë©”ì¼ ë°ì´í„°
            summary_type: ìš”ì•½ íƒ€ì… (brief, detailed, bullet, korean)
            include_attachments: ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼)
            only_attachments: Trueì¼ ê²½ìš° ì²¨ë¶€íŒŒì¼ë§Œ ìš”ì•½ (ë³¸ë¬¸ ì œì™¸)

        Returns:
            Dict: ìš”ì•½ ê²°ê³¼
        """
        try:
            # ì´ë©”ì¼ ë‚´ìš© êµ¬ì„±
            email_content_parts = []

            # ì œëª©
            if 'subject' in email_data:
                email_content_parts.append(f"ì œëª©: {email_data['subject']}")

            # ë°œì‹ ì
            if 'sender' in email_data:
                email_content_parts.append(f"ë°œì‹ ì: {email_data['sender']}")

            # ë‚ ì§œ
            if 'date' in email_data:
                email_content_parts.append(f"ë‚ ì§œ: {email_data['date']}")

            email_content_parts.append("=" * 50)

            # ë³¸ë¬¸ê³¼ ì²¨ë¶€íŒŒì¼ ì²˜ë¦¬
            if 'body' in email_data:
                body_text = email_data['body']

                # ë³¸ë¬¸ê³¼ ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ ë¶„ë¦¬
                if '=== ì²¨ë¶€íŒŒì¼:' in body_text:
                    body_parts = body_text.split('\n\n=== ì²¨ë¶€íŒŒì¼:')
                    body_only = body_parts[0]
                    attachment_text = '\n\n=== ì²¨ë¶€íŒŒì¼:'.join(body_parts[1:]) if len(body_parts) > 1 else ''

                    # ì˜µì…˜ì— ë”°ë¼ ë‚´ìš© ì„ íƒ
                    if only_attachments:
                        # ì²¨ë¶€íŒŒì¼ë§Œ ìš”ì•½
                        if attachment_text:
                            email_content_parts.append(f"\n[ì²¨ë¶€íŒŒì¼ ë‚´ìš©ë§Œ ìš”ì•½]\n=== ì²¨ë¶€íŒŒì¼:{attachment_text}")
                        else:
                            return {"error": "ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}
                    elif include_attachments:
                        # ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼
                        email_content_parts.append(body_only)
                        if attachment_text:
                            email_content_parts.append(f"\n=== ì²¨ë¶€íŒŒì¼:{attachment_text}")
                    else:
                        # ë³¸ë¬¸ë§Œ
                        email_content_parts.append(body_only)
                else:
                    # ì²¨ë¶€íŒŒì¼ êµ¬ë¶„ìê°€ ì—†ëŠ” ê²½ìš°
                    if only_attachments:
                        return {"error": "ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}
                    email_content_parts.append(body_text)

            email_content = '\n'.join(email_content_parts)

            # í† í° ìˆ˜ ê³„ì‚°
            token_count = self.count_tokens(email_content)

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í• 
            if token_count > self.max_tokens - 1000:
                print("ë©”ì¼ ë‚´ìš©ì´ ê¸¸ì–´ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
                chunks = self.split_text_by_tokens(email_content, max_chunk_tokens=3000)

                summaries = []
                for i, chunk in enumerate(chunks, 1):
                    print(f"ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ì¤‘...")
                    summary = self.summarize_chunk(chunk, summary_type)
                    summaries.append(summary)
                    time.sleep(1)

                combined_summary = "\n\n".join(summaries)
                if self.count_tokens(combined_summary) > 3000:
                    final_summary = self.summarize_chunk(combined_summary, summary_type)
                else:
                    final_summary = combined_summary
            else:
                final_summary = self.summarize_chunk(email_content, summary_type)

            # ìš”ì•½ ë²”ìœ„ í‘œì‹œ
            summary_scope = "ì²¨ë¶€íŒŒì¼ë§Œ" if only_attachments else ("ë³¸ë¬¸+ì²¨ë¶€íŒŒì¼" if include_attachments else "ë³¸ë¬¸ë§Œ")

            # ê²°ê³¼ ë°˜í™˜
            result = {
                "email_id": email_data.get('id', 'unknown'),
                "subject": email_data.get('subject', ''),
                "sender": email_data.get('sender', ''),
                "original_tokens": token_count,
                "summary": final_summary,
                "summary_tokens": self.count_tokens(final_summary),
                "compression_ratio": round(self.count_tokens(final_summary) / token_count * 100, 2),
                "summary_type": summary_type,
                "summary_scope": summary_scope,
                "included_attachments": include_attachments,
                "only_attachments": only_attachments
            }

            return result

        except Exception as e:
            return {"error": f"ë©”ì¼ ìš”ì•½ ì‹¤íŒ¨: {str(e)}"}

    def summarize_emails_batch(self, emails: List[Dict], summary_type: str = "detailed",
                               include_attachments: bool = True, only_attachments: bool = False,
                               delay: float = 1.0) -> List[Dict]:
        """
        ì—¬ëŸ¬ ë©”ì¼ì„ ì¼ê´„ ìš”ì•½

        Args:
            emails: íŒŒì‹±ëœ ì´ë©”ì¼ ë¦¬ìŠ¤íŠ¸
            summary_type: ìš”ì•½ íƒ€ì…
            include_attachments: ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼)
            only_attachments: Trueì¼ ê²½ìš° ì²¨ë¶€íŒŒì¼ë§Œ ìš”ì•½
            delay: API í˜¸ì¶œ ê°„ê²© (ì´ˆ)

        Returns:
            List[Dict]: ìš”ì•½ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []

        for i, email_data in enumerate(emails, 1):
            print(f"[{i}/{len(emails)}] ë©”ì¼ ìš”ì•½ ì¤‘: {email_data.get('subject', 'No Subject')[:50]}...")

            try:
                result = self.summarize_email(email_data, summary_type, include_attachments, only_attachments)
                results.append(result)

                if 'summary' in result:
                    print(f"âœ… ì™„ë£Œ (ì••ì¶•ë¥ : {result['compression_ratio']}%, ë²”ìœ„: {result.get('summary_scope', 'N/A')})")
                else:
                    print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")

                # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                if i < len(emails):
                    time.sleep(delay)

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                results.append({"error": str(e), "email_id": email_data.get('id', 'unknown')})

        return results

    def summarize_selected_emails(self, parser: 'NaverMailParser', email_ids: List[str],
                                  summary_type: str = "detailed",
                                  include_attachments: bool = True,
                                  only_attachments: bool = False) -> List[Dict]:
        """
        ì„ íƒëœ ë©”ì¼ë“¤ë§Œ ìš”ì•½ (NaverMailParser ì§ì ‘ ì‚¬ìš©)

        Args:
            parser: NaverMailParser ì¸ìŠ¤í„´ìŠ¤
            email_ids: ìš”ì•½í•  ì´ë©”ì¼ ID ë¦¬ìŠ¤íŠ¸
            summary_type: ìš”ì•½ íƒ€ì…
            include_attachments: ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ (ë³¸ë¬¸ + ì²¨ë¶€íŒŒì¼)
            only_attachments: Trueì¼ ê²½ìš° ì²¨ë¶€íŒŒì¼ë§Œ ìš”ì•½

        Returns:
            List[Dict]: ìš”ì•½ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """

        results = []

        for i, email_id in enumerate(email_ids, 1):
            print(f"[{i}/{len(email_ids)}] ë©”ì¼ ID {email_id} ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

            try:
                # ë©”ì¼ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                email_data = parser.view_email_content(email_id)

                if not email_data:
                    print(f"âŒ ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {email_id}")
                    results.append({"error": "ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ", "email_id": email_id})
                    continue

                # ìš”ì•½
                result = self.summarize_email(email_data, summary_type, include_attachments, only_attachments)
                results.append(result)

                if 'summary' in result:
                    print(f"âœ… ì™„ë£Œ (ì••ì¶•ë¥ : {result['compression_ratio']}%, ë²”ìœ„: {result.get('summary_scope', 'N/A')})")
                else:
                    print(f"âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")

                # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                if i < len(email_ids):
                    time.sleep(1.0)

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                results.append({"error": str(e), "email_id": email_id})

        return results

    def get_email_body_and_attachments_separately(self, parser: 'NaverMailParser', email_id: str) -> Dict:
        """
        NaverMailParserë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì¼ì˜ ë³¸ë¬¸ê³¼ ì²¨ë¶€íŒŒì¼ì„ ëª…í™•íˆ ë¶„ë¦¬

        Args:
            parser: NaverMailParser ì¸ìŠ¤í„´ìŠ¤
            email_id: ë©”ì¼ ID

        Returns:
            Dict: {
                'email_id': str,
                'subject': str,
                'sender': str,
                'date': str,
                'body_text': str,  # ìˆœìˆ˜ ë³¸ë¬¸
                'attachments': list  # ì²¨ë¶€íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (extracted_text í¬í•¨)
            }
        """

        try:
            # ë©”ì¼ ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            email_data = parser.view_email_content(email_id)

            if not email_data:
                return {"error": f"ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {email_id}"}

            # ë³¸ë¬¸ê³¼ ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ ë¶„ë¦¬
            body_text = email_data.get('body', '')
            body_only = body_text
            attachments_with_text = []

            # ì²¨ë¶€íŒŒì¼ êµ¬ë¶„ìë¡œ ë¶„ë¦¬
            if '=== ì²¨ë¶€íŒŒì¼:' in body_text:
                parts = body_text.split('\n\n=== ì²¨ë¶€íŒŒì¼:')
                body_only = parts[0].strip()

                # ê° ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for idx, part in enumerate(parts[1:], 1):
                    # íŒŒì¼ëª…ê³¼ ë‚´ìš© ë¶„ë¦¬
                    lines = part.split('\n', 1)
                    filename = lines[0].strip().replace('===', '').strip()
                    content = lines[1].strip() if len(lines) > 1 else ''

                    attachments_with_text.append({
                        'index': idx,
                        'filename': filename,
                        'extracted_text': content
                    })

            # ì²¨ë¶€íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if 'attachments' in email_data:
                for i, att_meta in enumerate(email_data['attachments']):
                    if i < len(attachments_with_text):
                        attachments_with_text[i].update(att_meta)

            return {
                'email_id': email_id,
                'subject': email_data.get('subject', ''),
                'sender': email_data.get('sender', ''),
                'date': email_data.get('date', ''),
                'recipient': email_data.get('recipient', ''),
                'body_text': body_only,
                'attachments': attachments_with_text,
                'has_attachments': len(attachments_with_text) > 0
            }

        except Exception as e:
            return {"error": f"ë©”ì¼ ë¶„ë¦¬ ì‹¤íŒ¨: {str(e)}"}

    def summarize_email_from_parser(self, parser: 'NaverMailParser', email_id: str,
                                    summary_type: str = "detailed",
                                    summarize_body: bool = True,
                                    summarize_attachments: bool = False) -> Dict:
        """
        NaverMailParserë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì¼ íŒŒì‹± í›„ ë³¸ë¬¸/ì²¨ë¶€íŒŒì¼ ì„ íƒ ìš”ì•½

        Args:
            parser: NaverMailParser ì¸ìŠ¤í„´ìŠ¤
            email_id: ë©”ì¼ ID
            summary_type: ìš”ì•½ íƒ€ì… (brief, detailed, bullet, korean)
            summarize_body: Trueì¼ ê²½ìš° ë³¸ë¬¸ ìš”ì•½
            summarize_attachments: Trueì¼ ê²½ìš° ì²¨ë¶€íŒŒì¼ ìš”ì•½

        Returns:
            Dict: ìš”ì•½ ê²°ê³¼
        """

        try:
            # ë©”ì¼ íŒŒì‹± - ë³¸ë¬¸ê³¼ ì²¨ë¶€íŒŒì¼ ë¶„ë¦¬
            print(f"ë©”ì¼ íŒŒì‹± ì¤‘ (ID: {email_id})...")
            separated_data = self.get_email_body_and_attachments_separately(parser, email_id)

            if 'error' in separated_data:
                return separated_data

            results = {
                'email_id': separated_data['email_id'],
                'subject': separated_data['subject'],
                'sender': separated_data['sender'],
                'date': separated_data['date'],
                'has_attachments': separated_data['has_attachments']
            }

            # ë³¸ë¬¸ ìš”ì•½
            if summarize_body and separated_data['body_text'].strip():
                print("ğŸ“ ë³¸ë¬¸ ìš”ì•½ ì¤‘...")
                body_content = f"ì œëª©: {results['subject']}\në°œì‹ ì: {results['sender']}\n{'='*50}\n{separated_data['body_text']}"

                body_token_count = self.count_tokens(body_content)
                body_summary = self.summarize_chunk(body_content, summary_type)

                results['body_summary'] = body_summary
                results['body_original_length'] = len(separated_data['body_text'])
                results['body_tokens'] = body_token_count
                results['body_summary_tokens'] = self.count_tokens(body_summary)
                results['body_compression_ratio'] = round(self.count_tokens(body_summary) / body_token_count * 100, 2)
                print(f"âœ… ë³¸ë¬¸ ìš”ì•½ ì™„ë£Œ (ì••ì¶•ë¥ : {results['body_compression_ratio']}%)")
            else:
                results['body_summary'] = None

            # ì²¨ë¶€íŒŒì¼ ìš”ì•½
            if summarize_attachments and separated_data['attachments']:
                attachment_count = len(separated_data['attachments'])
                print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ ìš”ì•½ ì¤‘ ({attachment_count}ê°œ)...")
                attachment_summaries = []

                for att in separated_data['attachments']:
                    idx = att['index']
                    filename = att['filename']
                    extracted_text = att.get('extracted_text', '')

                    if not extracted_text.strip():
                        print(f"  âš ï¸  ì²¨ë¶€íŒŒì¼ {idx} ({filename}): ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì—†ìŒ")
                        attachment_summaries.append({
                            'index': idx,
                            'filename': filename,
                            'summary': '[í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” íŒŒì¼ì…ë‹ˆë‹¤]',
                            'original_tokens': 0,
                            'summary_tokens': 0
                        })
                        continue

                    print(f"  ğŸ“„ ì²¨ë¶€íŒŒì¼ {idx}/{attachment_count} ({filename}) ìš”ì•½ ì¤‘...")
                    att_content = f"ì²¨ë¶€íŒŒì¼: {filename}\n{'='*50}\n{extracted_text}"
                    att_token_count = self.count_tokens(att_content)
                    att_summary = self.summarize_chunk(att_content, summary_type)

                    attachment_summaries.append({
                        'index': idx,
                        'filename': filename,
                        'summary': att_summary,
                        'original_length': len(extracted_text),
                        'original_tokens': att_token_count,
                        'summary_tokens': self.count_tokens(att_summary),
                        'compression_ratio': round(self.count_tokens(att_summary) / att_token_count * 100, 2) if att_token_count > 0 else 0
                    })
                    print(f"  âœ… ì™„ë£Œ (ì••ì¶•ë¥ : {attachment_summaries[-1]['compression_ratio']}%)")
                    time.sleep(0.5)  # API ì œí•œ ë°©ì§€

                results['attachment_summaries'] = attachment_summaries
                results['attachment_count'] = len(attachment_summaries)
                print(f"âœ… ì „ì²´ ì²¨ë¶€íŒŒì¼ ìš”ì•½ ì™„ë£Œ")
            else:
                results['attachment_summaries'] = []
                results['attachment_count'] = 0

            # ì „ì²´ í†µí•© ìš”ì•½ ìƒì„±
            if summarize_body and summarize_attachments and results.get('body_summary') and results.get('attachment_summaries'):
                print("ğŸ”„ í†µí•© ìš”ì•½ ìƒì„± ì¤‘...")
                combined_parts = []

                combined_parts.append(f"[ë³¸ë¬¸ ìš”ì•½]\n{results['body_summary']}")

                for att_sum in results['attachment_summaries']:
                    if att_sum['summary'] != '[í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ëŠ” íŒŒì¼ì…ë‹ˆë‹¤]':
                        combined_parts.append(f"[ì²¨ë¶€íŒŒì¼: {att_sum['filename']}]\n{att_sum['summary']}")

                results['combined_summary'] = '\n\n'.join(combined_parts)
                print("âœ… í†µí•© ìš”ì•½ ì™„ë£Œ")

            # ì „ì²´ í†µê³„
            total_original_tokens = results.get('body_tokens', 0)
            total_summary_tokens = results.get('body_summary_tokens', 0)

            if results.get('attachment_summaries'):
                for att in results['attachment_summaries']:
                    total_original_tokens += att.get('original_tokens', 0)
                    total_summary_tokens += att.get('summary_tokens', 0)

            results['total_original_tokens'] = total_original_tokens
            results['total_summary_tokens'] = total_summary_tokens
            results['total_compression_ratio'] = round(total_summary_tokens / total_original_tokens * 100, 2) if total_original_tokens > 0 else 0

            return results

        except Exception as e:
            return {"error": f"ìš”ì•½ ì‹¤íŒ¨: {str(e)}"}

    def summarize_multiple_emails_from_parser(self, parser: 'NaverMailParser',
                                              criteria: str = 'ALL',
                                              limit: int = 5,
                                              summary_type: str = "detailed",
                                              summarize_body: bool = True,
                                              summarize_attachments: bool = False) -> List[Dict]:
        """
        NaverMailParserë¡œ ì—¬ëŸ¬ ë©”ì¼ íŒŒì‹± í›„ ì„ íƒ ìš”ì•½

        Args:
            parser: NaverMailParser ì¸ìŠ¤í„´ìŠ¤
            criteria: ê²€ìƒ‰ ê¸°ì¤€ ('ALL', 'UNSEEN', 'SEEN' ë“±)
            limit: ê°€ì ¸ì˜¬ ë©”ì¼ ê°œìˆ˜
            summary_type: ìš”ì•½ íƒ€ì…
            summarize_body: ë³¸ë¬¸ ìš”ì•½ ì—¬ë¶€
            summarize_attachments: ì²¨ë¶€íŒŒì¼ ìš”ì•½ ì—¬ë¶€

        Returns:
            List[Dict]: ìš”ì•½ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """

        try:
            # ë©”ì¼ ê²€ìƒ‰
            print(f"ë©”ì¼ ê²€ìƒ‰ ì¤‘ (ê¸°ì¤€: {criteria}, ê°œìˆ˜: {limit})...")
            email_ids = parser.search_emails(criteria=criteria, limit=limit)

            if not email_ids:
                return []

            print(f"{len(email_ids)}ê°œ ë©”ì¼ ë°œê²¬")

            results = []
            for i, email_id in enumerate(email_ids, 1):
                email_id_str = email_id.decode() if isinstance(email_id, bytes) else str(email_id)
                print(f"\n[{i}/{len(email_ids)}] ë©”ì¼ ì²˜ë¦¬ ì¤‘ (ID: {email_id_str})...")

                result = self.summarize_email_from_parser(
                    parser,
                    email_id_str,
                    summary_type,
                    summarize_body,
                    summarize_attachments
                )

                results.append(result)

                # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                if i < len(email_ids):
                    time.sleep(1.0)

            return results

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            return [{"error": str(e)}]
    
    def summarize_downloaded_attachment(self, attachment_path: str, summary_type: str = "detailed") -> Dict:
        """
        NaverMailParserë¡œ ë‹¤ìš´ë¡œë“œ ë°›ì€ ì²¨ë¶€íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ìš”ì•½

        Args:
            attachment_path: ì²¨ë¶€íŒŒì¼ ê²½ë¡œ
            summary_type: ìš”ì•½ íƒ€ì… (brief, detailed, bullet, korean)

        Returns:
            Dict: ìš”ì•½ ê²°ê³¼
        """
        try:
            # NaverMailParserì˜ extract_text_from_file ì‚¬ìš©
            parser = NaverMailParser(username="dummy", password="dummy")

            print(f"ğŸ“„ ì²¨ë¶€íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {Path(attachment_path).name}")
            extracted_text = parser.extract_text_from_file(attachment_path)

            # ì¶”ì¶œ ì‹¤íŒ¨ í™•ì¸
            if extracted_text.startswith('[') and 'ì‹¤íŒ¨' in extracted_text or 'ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤' in extracted_text:
                return {
                    'file_path': attachment_path,
                    'filename': Path(attachment_path).name,
                    'error': extracted_text
                }

            # í† í° ìˆ˜ ê³„ì‚°
            token_count = self.count_tokens(extracted_text)
            print(f"ğŸ“Š ì¶”ì¶œëœ í…ìŠ¤íŠ¸ í† í° ìˆ˜: {token_count:,}")

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸´ ê²½ìš° ë¶„í• 
            if token_count > self.max_tokens - 1000:
                print("í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤...")
                chunks = self.split_text_by_tokens(extracted_text, max_chunk_tokens=3000)

                summaries = []
                for i, chunk in enumerate(chunks, 1):
                    print(f"ì²­í¬ {i}/{len(chunks)} ìš”ì•½ ì¤‘...")
                    summary = self.summarize_chunk(chunk, summary_type)
                    summaries.append(summary)
                    time.sleep(1)

                combined_summary = "\n\n".join(summaries)
                if self.count_tokens(combined_summary) > 3000:
                    final_summary = self.summarize_chunk(combined_summary, summary_type)
                else:
                    final_summary = combined_summary
            else:
                print("ë‹¨ì¼ ìš”ì•½ ìƒì„± ì¤‘...")
                final_summary = self.summarize_chunk(extracted_text, summary_type)

            # ê²°ê³¼ ë°˜í™˜
            result = {
                'file_path': attachment_path,
                'filename': Path(attachment_path).name,
                'file_size': os.path.getsize(attachment_path),
                'extracted_text_length': len(extracted_text),
                'original_tokens': token_count,
                'summary': final_summary,
                'summary_tokens': self.count_tokens(final_summary),
                'compression_ratio': round(self.count_tokens(final_summary) / token_count * 100, 2) if token_count > 0 else 0,
                'summary_type': summary_type
            }

            print(f"âœ… ìš”ì•½ ì™„ë£Œ (ì••ì¶•ë¥ : {result['compression_ratio']}%)")
            return result

        except Exception as e:
            return {
                'file_path': attachment_path,
                'filename': Path(attachment_path).name,
                'error': f"ì²¨ë¶€íŒŒì¼ ìš”ì•½ ì‹¤íŒ¨: {str(e)}"
            }

    def summarize_email_attachments_from_path(self, email_folder_path: str, summary_type: str = "detailed") -> Dict:
        """
        NaverMailParserë¡œ ë‹¤ìš´ë¡œë“œëœ ì´ë©”ì¼ í´ë”ì˜ ëª¨ë“  ì²¨ë¶€íŒŒì¼ì„ íŒŒì‹±í•˜ê³  ìš”ì•½

        Args:
            email_folder_path: ì´ë©”ì¼ ë‹¤ìš´ë¡œë“œ í´ë” ê²½ë¡œ (ì˜ˆ: downloads/email_123)
            summary_type: ìš”ì•½ íƒ€ì… (brief, detailed, bullet, korean)

        Returns:
            Dict: ì „ì²´ ìš”ì•½ ê²°ê³¼
        """
        try:
            email_folder = Path(email_folder_path)

            if not email_folder.exists():
                return {'error': f'í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {email_folder_path}'}

            # ë©”íƒ€ë°ì´í„° ì½ê¸°
            metadata_file = email_folder / 'metadata.json'
            metadata = {}
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

            # ë³¸ë¬¸ ì½ê¸°
            body_text = ""
            text_file = email_folder / 'body.txt'
            if text_file.exists():
                with open(text_file, 'r', encoding='utf-8') as f:
                    body_text = f.read()

            # ì²¨ë¶€íŒŒì¼ ì°¾ê¸° (body.txt, body.html, metadata.json ì œì™¸)
            attachments = []
            for file in email_folder.iterdir():
                if file.name not in ['body.txt', 'body.html', 'metadata.json'] and file.is_file():
                    attachments.append(file)

            if not attachments:
                return {
                    'email_id': metadata.get('id', 'unknown'),
                    'subject': metadata.get('subject', ''),
                    'attachment_count': 0,
                    'message': 'ì²¨ë¶€íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤'
                }

            print(f"ğŸ“§ ì´ë©”ì¼ í´ë”: {email_folder.name}")
            print(f"ğŸ“ ì²¨ë¶€íŒŒì¼ {len(attachments)}ê°œ ë°œê²¬")

            # ê° ì²¨ë¶€íŒŒì¼ ìš”ì•½
            attachment_summaries = []
            for i, att_path in enumerate(attachments, 1):
                print(f"\n[{i}/{len(attachments)}] {att_path.name} ì²˜ë¦¬ ì¤‘...")

                result = self.summarize_downloaded_attachment(str(att_path), summary_type)
                attachment_summaries.append(result)

                if i < len(attachments):
                    time.sleep(0.5)  # API ì œí•œ ë°©ì§€

            # ì „ì²´ ê²°ê³¼
            total_original_tokens = sum(r.get('original_tokens', 0) for r in attachment_summaries if 'error' not in r)
            total_summary_tokens = sum(r.get('summary_tokens', 0) for r in attachment_summaries if 'error' not in r)

            return {
                'email_id': metadata.get('id', 'unknown'),
                'subject': metadata.get('subject', ''),
                'sender': metadata.get('sender', ''),
                'date': metadata.get('date', ''),
                'folder_path': str(email_folder),
                'attachment_count': len(attachments),
                'attachment_summaries': attachment_summaries,
                'total_original_tokens': total_original_tokens,
                'total_summary_tokens': total_summary_tokens,
                'total_compression_ratio': round(total_summary_tokens / total_original_tokens * 100, 2) if total_original_tokens > 0 else 0,
                'summary_type': summary_type
            }

        except Exception as e:
            return {'error': f'ì´ë©”ì¼ ì²¨ë¶€íŒŒì¼ ìš”ì•½ ì‹¤íŒ¨: {str(e)}'}

    def batch_summarize(self, input_dir: str, output_dir: str = None,
                       summary_type: str = "detailed", file_pattern: str = "*.txt"):
        """ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ ì¼ê´„ ìš”ì•½"""
        input_path = Path(input_dir)
        output_path = Path(output_dir) if output_dir else input_path / "summaries"
        output_path.mkdir(exist_ok=True)

        txt_files = list(input_path.glob(file_pattern))

        if not txt_files:
            print(f"{input_dir}ì—ì„œ {file_pattern} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"{len(txt_files)}ê°œ íŒŒì¼ ìš”ì•½ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        results = []
        for i, file_path in enumerate(txt_files, 1):
            print(f"\n[{i}/{len(txt_files)}] {file_path.name} ìš”ì•½ ì¤‘...")

            output_file = output_path / f"{file_path.stem}_summary.txt"
            result = self.summarize_file(str(file_path), summary_type, str(output_file))

            if "error" not in result:
                print(f"âœ… ì™„ë£Œ: {output_file}")
                results.append(result)
            else:
                print(f"âŒ ì‹¤íŒ¨: {result['error']}")

        # ì „ì²´ ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary_report = output_path / "batch_summary_report.json"
        with open(summary_report, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nì¼ê´„ ìš”ì•½ ì™„ë£Œ! ê²°ê³¼: {output_path}")

def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='í…ìŠ¤íŠ¸ íŒŒì¼ LLM ìš”ì•½ê¸°')
    parser.add_argument('file_path', help='ìš”ì•½í•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--type', '-t', choices=['brief', 'detailed', 'bullet', 'korean'], 
                       default='detailed', help='ìš”ì•½ íƒ€ì…')
    parser.add_argument('--model', '-m', default='gpt-3.5-turbo', help='ì‚¬ìš©í•  ëª¨ë¸')
    parser.add_argument('--batch', '-b', help='ë””ë ‰í† ë¦¬ ì¼ê´„ ì²˜ë¦¬')
    
    args = parser.parse_args()
    
    # API í‚¤ í™•ì¸
    if not os.getenv('OPENAI_API_KEY'):
        print("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("ì˜ˆ: export OPENAI_API_KEY='your-api-key-here'")
        return
    
    summarizer = TextSummarizer(model=args.model)
    
    try:
        if args.batch:
            # ì¼ê´„ ì²˜ë¦¬
            summarizer.batch_summarize(args.batch, summary_type=args.type)
        else:
            # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
            result = summarizer.summarize_file(args.file_path, args.type, args.output)
            
            if "error" in result:
                print(f"âŒ {result['error']}")
            else:
                print(f"\nâœ… ìš”ì•½ ì™„ë£Œ!")
                print(f"ì••ì¶•ë¥ : {result['compression_ratio']}%")
                print(f"ì›ë³¸: {result['original_tokens']:,} í† í° â†’ ìš”ì•½: {result['summary_tokens']:,} í† í°")
                print("\n" + "="*50)
                print(result['summary'])
                
                if args.output:
                    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {args.output}")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì˜ˆì‹œ
    # summarizer = TextSummarizer()
    # result = summarizer.summarize_file("example.txt", "korean", "summary.txt")
    # print(result)
    
    main()